{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II - Model building\n",
    "\n",
    "In this notebook, you will...\n",
    "* Build a baseline model to compare all of your other models against\n",
    "* Build an ensemble model (Gradient Boosting Regression)\n",
    "* Build a simple neural network\n",
    "\n",
    "\n",
    "You will also look at:\n",
    "* Feature testing - how does your baseline model change with the various features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, explained_variance_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import itertools\n",
    "import json\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = Path.cwd()\n",
    "root_dir = current_dir.parent\n",
    "data_dir = Path(current_dir, \"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the groups of sensors as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ops = ['Unit_4_Power', 'Unit_4_Reactive Power', 'Turbine_Guide Vane Opening',\n",
    "       'Turbine_Pressure Drafttube', 'Turbine_Pressure Spiral Casing',\n",
    "       'Turbine_Rotational Speed', 'mode', 'Bolt_1_Steel tmp']\n",
    "\n",
    "bolt_temp = ['Bolt_1_Steel tmp']\n",
    "\n",
    "bolt_tensiles = ['Bolt_1_Tensile', 'Bolt_2_Tensile',  'Bolt_3_Tensile', \n",
    "                 'Bolt_4_Tensile', 'Bolt_5_Tensile',  'Bolt_6_Tensile']\n",
    "\n",
    "bolt_torsions = ['Bolt_1_Torsion',  'Bolt_2_Torsion', 'Bolt_3_Torsion',\n",
    "                 'Bolt_4_Torsion', 'Bolt_5_Torsion',  'Bolt_6_Torsion']\n",
    "\n",
    "vibrations = ['lower_bearing_vib_vrt', 'turbine_bearing_vib_vrt'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the data. We will use the input dataset 2 since we know this is the most stable dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = pd.read_parquet(Path(data_dir, \"input_dataset-2.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the input dataframe for modelling. \n",
    "* If we have any NaNs in the model inputs and model outputs, we should either delete them or process them. Let's start by seeing how many NaNs there are.\n",
    "* __```TODO``` - this work should be moved to the data exploration and preparation notebook__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The original length of the dataframe is {} rows\".format(input_df.shape))\n",
    "print(\"If we drop NaNs in the subset of inputs and outputs, we end up with {} rows\".format(\n",
    "    input_df.dropna(subset=ops+bolt_tensiles).shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That still leaves us with an acceptable number of rows for training, so let's go ahead and drop the NaNs in the subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = input_df.dropna(subset=ops+bolt_tensiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make the mode non-categorical for easier model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df['mode'], codes = pd.factorize(input_df['mode'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare our data for training and testing.\n",
    "First, we'll separate into X and y and then divide into train and test. We'll make the test size about 10%, using a random state of 0 and no shuffle so that we end up with data that is continuous in time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our ```X``` are the operating conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = ops\n",
    "X = input_df[model_inputs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our ```Y``` are the bolt tensiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_outputs = bolt_tensiles\n",
    "y = input_df[model_outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's make a baseline model to evaluate against.\n",
    "\n",
    "We'll use a Min Max Scaler and a Linear Regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create the pipeline\n",
    "* Fit the baseline model\n",
    "* Predict using the trained baseline model\n",
    "* Compare predictions and actuals using mean squared error and mean absolute error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_baseline = make_pipeline(MinMaxScaler(), LinearRegression())\n",
    "\n",
    "predictions = []\n",
    "scores = {}\n",
    "\n",
    "pipeline_baseline.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipeline_baseline.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "evs = explained_variance_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error: {} \\nMean Absolute Error: {} \\nExplained Variance Score: {}\".format(mse, mae, evs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Is this good or bad? Let's visualize the actuals and predicted to see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_df = pd.DataFrame(y_pred, index = y_test.index)\n",
    "y_pred_df.columns=y_test.columns\n",
    "    \n",
    "fig, axes = plt.subplots(nrows=6, ncols=1, figsize=(20, 20))\n",
    "\n",
    "for t, ax in zip(bolt_tensiles, axes):\n",
    "    ax.scatter(y_test.index, \n",
    "               y_test[t], label=t, color='r',\n",
    "               s=1, alpha=0.1)\n",
    "\n",
    "    ax.scatter(y_pred_df.index, \n",
    "               y_pred_df[t], label=t, color='b',\n",
    "               s=1, alpha=0.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's make some helper functions to quickly train and score different models, and one to plot actuals vs predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train_score(X_train, X_test, y_train, y_test, model_pipeline):\n",
    "    \n",
    "    predictions = []\n",
    "    for y_ in y_train:\n",
    "        print(\"Training {}...\".format(y_))\n",
    "        model_pipeline.fit(X_train, y_train[y_])\n",
    "        y_pred_ = model_pipeline.predict(X_test)\n",
    "        predictions.append(pd.Series(y_pred_, index = X_test.index, name=y_))\n",
    "\n",
    "    y_pred_df = pd.concat(predictions, axis=1)\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred_df)\n",
    "    mae = mean_absolute_error(y_test, y_pred_df)\n",
    "    evs = explained_variance_score(y_test, y_pred_df)\n",
    "    \n",
    "    #y_pred_df.columns=y_test.columns\n",
    "    \n",
    "    return [mse, mae, evs], y_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_actuals_vs_predicted(df_actuals, df_predictions, show=True, save_as=False):\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=6, ncols=1, figsize=(20, 20))\n",
    "\n",
    "    for t, ax in zip(bolt_tensiles, axes):\n",
    "        ax.scatter(df_actuals.index, \n",
    "                   df_actuals[t], label=t, color='r',\n",
    "                   s=1, alpha=0.1)\n",
    "\n",
    "        ax.scatter(df_predictions.index, \n",
    "                   df_predictions[t], label=t, color='b',\n",
    "                   s=1, alpha=0.1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if show:\n",
    "        plt.show()\n",
    "    \n",
    "    if save_as:\n",
    "        plt.savefig(save_as)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering\n",
    "\n",
    "There seems to be a bit of an offset between the predictions and the actuals. This might be accomodated by looking at a ```time_since_start``` feature. This should help to capture the linear increase we're observing over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df[\"time_since_start\"] = input_df.index - input_df.index[0]\n",
    "input_df[\"time_since_start\"] = input_df[\"time_since_start\"].dt.total_seconds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does including this into the model train improve the performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = ops+[\"time_since_start\"]\n",
    "outputs = bolt_tensiles\n",
    "\n",
    "X = input_df[inputs]\n",
    "y = input_df[outputs]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0, shuffle=False)\n",
    "\n",
    "pipeline = make_pipeline(MinMaxScaler(), LinearRegression())\n",
    "\n",
    "test = model_train_score(X_train, X_test, y_train, y_test, pipeline)\n",
    "scores, predictions_df = model_train_score(X_train, X_test, y_train, y_test, pipeline)\n",
    "\n",
    "print(\"Mean Squared Error: {} \\nMean Absolute Error: {} \\nExplained Variance Score: {}\".format(scores[0], \n",
    "                                                                                               scores[1], \n",
    "                                                                                               scores[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_actuals_vs_predicted(y_test, predictions_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better - this seems to capture a lot of that offset we were seeing in the previous iteration. Let's also add a couple of other features and see if they allow us to minimize the gaps at the beginning of each start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Features that we can look at...\n",
    "\n",
    "__```TODO``` - this should be moved to the data exploration and preparation__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ```time_in_operation``` - this could help us to evaluate how time running affects the temperature and behaviour of the bolts\n",
    "* ```time_since_last_stop``` - this could help us to evaluate how cold the system is when it's starting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mode for operation is 0 and the mode for start is 1 (based on the codes generated during the factorization earlier).\n",
    "\n",
    "We will then first identify each running window:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find starts. Start = 1; So all starts will be where the difference == 1\n",
    "input_df['temporary_mode_diff'] = input_df['mode'].diff()\n",
    "input_df.loc[input_df.temporary_mode_diff == 1, \"temporary_operation_starts_ends\"] = \"start\"\n",
    "\n",
    "# Find ends. Shift the temporary_mode_diff up one, because right before each start is the end of the last operation\n",
    "input_df['temporary_mode_diff_ends'] = input_df['temporary_mode_diff'].shift(-1)\n",
    "input_df.loc[input_df.temporary_mode_diff_ends == 1, \"temporary_operation_starts_ends\"] = \"end\"\n",
    "\n",
    "# Wherever there is an end of operation, save the time. Wherever there is a start, save the time.\n",
    "input_df.loc[input_df.temporary_operation_starts_ends == \"end\", \"end_of_operation\"] = input_df.index.to_series()\n",
    "input_df.loc[input_df.temporary_operation_starts_ends == \"start\", \"start_of_operation\"] = input_df.index.to_series()\n",
    "\n",
    "# We forward fill the end of operation so we can say how long it's been since the last operational mode.\n",
    "input_df[\"time_since_last_stop\"] = input_df[\"end_of_operation\"].ffill()\n",
    "\n",
    "# We forward fill the start of operation so we can say how long it's been since the current operational mode started.\n",
    "input_df[\"time_in_operation\"] = input_df[\"start_of_operation\"].ffill()\n",
    "\n",
    "# Subtract this time from the index to get the actual time since last operation\n",
    "input_df[\"time_since_last_stop\"] = input_df.index.to_series() - input_df[\"time_since_last_stop\"]\n",
    "\n",
    "# Subtract the time from the index to get the actual time running\n",
    "input_df[\"time_in_operation\"] = input_df.index.to_series() - input_df[\"time_in_operation\"]\n",
    "\n",
    "# Convert to seconds\n",
    "input_df[\"time_since_last_stop\"] = input_df[\"time_since_last_stop\"].dt.total_seconds()\n",
    "input_df[\"time_in_operation\"] = input_df[\"time_in_operation\"].dt.total_seconds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple models & scaling methods\n",
    "\n",
    "### REGRESSION\n",
    "\n",
    "#### Let's look at 3 scaling methods (MinMax, Standard, and None) in conjunction with 2 prediction methods (a linear and an ensemble approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling_methods = [MinMaxScaler(),\n",
    "                   StandardScaler(),\n",
    "                   None]\n",
    "\n",
    "prediction_methods = [LinearRegression(),\n",
    "                      GradientBoostingRegressor()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create tuples of all possible combinations of the scaling and prediction methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_combinations = list(itertools.product(scaling_methods, prediction_methods))\n",
    "\n",
    "# Let's just print the list to verify what comes out.\n",
    "pipeline_combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create sklearn pipelines for the combinations. \n",
    "\n",
    "__Note:__ the ```GradientBoostingRegressor``` takes some time to run, so you can first evaluate the effect of the scalers against the baseline model ```LinearRegression```, before you start looking at different prediction methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pipelines = [make_pipeline(*x) for x in pipeline_combinations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output_dir = Path(current_dir, \"model_outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores = {}\n",
    "all_predictions = []\n",
    "for p in all_pipelines:\n",
    "    p_name = \"_\".join(p.named_steps.keys())\n",
    "    print(p_name)\n",
    "    \n",
    "    scores, predictions_df = model_train_score(X_train, X_test, y_train, y_test, p)\n",
    "    all_scores[p_name] = scores\n",
    "    print(all_scores)\n",
    "    \n",
    "    # Save the predictions and the plots\n",
    "    predictions_df.to_csv(Path(model_output_dir, \"{}.csv\".format(p_name)))\n",
    "    \n",
    "    plot_actuals_vs_predicted(y_test, \n",
    "                              predictions_df, \n",
    "                              show=False, \n",
    "                              save_as = Path(model_output_dir, \"{}.jpeg\".format(p_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the scores\n",
    "pd.DataFrame(all_scores).T.sort_values(by=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM\n",
    "\n",
    "#### ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
